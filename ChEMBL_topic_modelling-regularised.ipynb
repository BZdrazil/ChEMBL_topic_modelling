{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfce69b",
   "metadata": {},
   "source": [
    "# Wordcloud by using the PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd623e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "# Enter your email address (required for using the Entrez API)\n",
    "Entrez.email = \"\"\n",
    "\n",
    "# Search for papers related to ChEMBL\n",
    "search_query = \"(ChEMBL[Title/Abstract]) AND 2010:2024[Date - Publication]\"\n",
    "\n",
    "# Use the Entrez API to search PubMed\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_query, retmax=1000)  # retmax specifies the max number of records\n",
    "record = Entrez.read(handle)\n",
    "\n",
    "# Fetch the PubMed IDs of the articles\n",
    "pmids = record[\"IdList\"]\n",
    "\n",
    "# Calculate and display the number of papers\n",
    "num_papers = len(pmids)\n",
    "print(f\"Number of papers retrieved: {num_papers}\")\n",
    "\n",
    "# Fetch detailed information (including abstracts) using the PubMed IDs\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), retmode=\"xml\", rettype=\"abstract\")\n",
    "records = Entrez.read(handle)\n",
    "\n",
    "# Extract and print the abstracts\n",
    "abstracts = []\n",
    "for r in records[\"PubmedArticle\"]:\n",
    "    try:\n",
    "        # Extract the title and abstract\n",
    "        title = r[\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
    "        abstract = r[\"MedlineCitation\"][\"Article\"][\"Abstract\"][\"AbstractText\"][0]\n",
    "        abstracts.append(f\"Title: {title}\\nAbstract: {abstract}\\n\")\n",
    "    except KeyError:\n",
    "        # Handle cases where no abstract is available\n",
    "        abstracts.append(f\"Title: {title}\\nAbstract: Not available\\n\")\n",
    "\n",
    "# Print the abstracts\n",
    "for abstract in abstracts:\n",
    "    print(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1432b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Function to preprocess and filter out stopwords (including custom ones)\n",
    "def process_text(text_list):\n",
    "    # Join all the abstracts into one large string\n",
    "    full_text = ' '.join(text_list).lower()\n",
    "    \n",
    "    # Tokenize the text (split into words)\n",
    "    tokens = nltk.word_tokenize(full_text)\n",
    "    \n",
    "    # Custom stopwords to remove (e.g., 'title', 'abstract')\n",
    "    custom_stopwords = set(['however', 'developed','identified','development','approach', 'results','small','one','two','sub','based','compounds','discovery','search','also', 'available','databases','database','chemical', 'model', 'compound','used','chembl','title', 'abstract', 'data', 'information', 'study', 'analysis', 'different', 'using', 'sets', 'set','method', 'new'])\n",
    "    \n",
    "    # Combine NLTK's stopwords with your custom stopwords\n",
    "    stop_words = set(stopwords.words('english')).union(custom_stopwords)\n",
    "    \n",
    "    # Filter out stopwords and non-alphabetical tokens\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Function to count most common keywords\n",
    "def get_most_common_keywords(tokens, n=30):\n",
    "    freq = Counter(tokens)\n",
    "    return freq.most_common(n)\n",
    "\n",
    "# Function to visualize the most common keywords as a word cloud\n",
    "# Enter path to file and filename\n",
    "def visualize_keywords(keywords, filename=''):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(keywords))\n",
    "    \n",
    "    # Save the word cloud image to a file\n",
    "    wordcloud.to_file(filename)\n",
    "    print(f\"Word cloud saved successfully at: {filename}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Process text to remove common stopwords (including 'title' and 'abstract')\n",
    "tokens = process_text(abstracts)\n",
    "\n",
    "# Get the most common keywords\n",
    "common_keywords = get_most_common_keywords(tokens, 30)\n",
    "\n",
    "\n",
    "# Visualize the keywords\n",
    "visualize_keywords(common_keywords)\n",
    "\n",
    "\n",
    "# Output most common keywords\n",
    "print(common_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ff149",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib==3.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52277775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configure Entrez API\n",
    "Entrez.email = \"\"  \n",
    "\n",
    "# Predefined labels for possible topics\n",
    "labels = [\n",
    "    \"Machine Learning\", \"Artificial Intelligence\", \"Drug Inhibition\", \"Prediction\", \"Structure-Activity\",\n",
    "     \"Similarity\",  \"SARSCOV2\", \"Screening\",\n",
    "    \"Deep Learning\", \"QSAR\", \"Toxicity\", \"Repurposing\", \"Infectious Disease\", \"Neurodegenerative Disease\", \"Antibiotic Resistance\", \"Antimicrobial\",\n",
    "    \"Molecular Modeling\", \"Precision Medicine\", \"Personalized Medicine\",\n",
    "     \"Target-Based Drug Discovery\"\n",
    "]\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Adjust time period\n",
    "def search_pubmed(term, max_results=1000):\n",
    "    \"\"\"Search PubMed for a given term and return a list of PubMed IDs, filtering by publication date (2010-2019).\"\"\"\n",
    "    query = f\"{term} AND ({'2010/01/01'[0:4]}[PDAT] : {'2019/12/31'[0:4]}[PDAT])\"\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record[\"IdList\"]\n",
    "\n",
    "\n",
    "def fetch_pubmed_details(pubmed_ids):\n",
    "    \"\"\"Fetch details for a list of PubMed IDs.\"\"\"\n",
    "    ids = \",\".join(pubmed_ids)\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=ids, rettype=\"medline\", retmode=\"text\")\n",
    "    records = handle.read()\n",
    "    handle.close()\n",
    "    return records\n",
    "\n",
    "\n",
    "def extract_titles_and_abstracts(records):\n",
    "    \"\"\"Extract titles and abstracts from PubMed records.\"\"\"\n",
    "    papers = []\n",
    "    for record in records.split('\\n\\n'):\n",
    "        title_match = re.search(r\"TI  - (.+)\", record)\n",
    "        abstract_match = re.search(r\"AB  - (.+)\", record)\n",
    "        title = title_match.group(1) if title_match else \"\"\n",
    "        abstract = abstract_match.group(1) if abstract_match else \"\"\n",
    "        if title or abstract:\n",
    "            papers.append(title + \" \" + abstract)\n",
    "    return papers\n",
    "\n",
    "\n",
    "def preprocess_text(texts, exclude_words=None):\n",
    "    \"\"\"Preprocess text for topic modeling and exclude certain words.\"\"\"\n",
    "    if exclude_words is None:\n",
    "        exclude_words = []  # Default to empty list if no words are specified for exclusion\n",
    "    \n",
    "    # Basic cleanup\n",
    "    texts = [re.sub(r\"[^a-zA-Z ]\", \"\", text.lower()) for text in texts]\n",
    "    \n",
    "    # Remove the excluded words\n",
    "    texts = [\n",
    "        \" \".join([word for word in text.split() if word not in exclude_words])\n",
    "        for text in texts\n",
    "    ]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "def perform_topic_modeling(texts, num_topics=5, num_words=10):\n",
    "    \"\"\"Perform topic modeling on a list of texts.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\", \n",
    "        max_features=1000,\n",
    "        max_df=0.85,          # Remove words appearing in over 85% of documents\n",
    "        min_df=10              # Remove words appearing in fewer than 10 documents\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics, \n",
    "        random_state=42,\n",
    "        learning_method='online',   \n",
    "        learning_decay=0.7,         # Decay the learning rate over iterations\n",
    "        learning_offset=50.0,       # Offset to prevent rapid learning\n",
    "        doc_topic_prior=0.1,        # Regularization parameter for topics per document\n",
    "        topic_word_prior=0.1        # Regularization parameter for words per topic\n",
    "    )\n",
    "    lda.fit(tfidf_matrix)\n",
    "\n",
    "      # Post-processing step: Clip weights to limit extremes\n",
    "    # Clip the topic-word distributions to limit values above 0.1, for example\n",
    "    topic_word_distributions = np.clip(lda.components_, 0, 0.1)\n",
    "\n",
    "    topics = {}\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-num_words:]]\n",
    "        topics[f\"Topic {idx + 1}\"] = words\n",
    "\n",
    "    return lda, tfidf_matrix, vectorizer, topics\n",
    "\n",
    "\n",
    "def get_topic_label(top_words, model, labels):\n",
    "    \"\"\"Generate a representative label for a topic using predefined labels and word embeddings.\"\"\"\n",
    "    word_embeddings = model.encode(top_words)\n",
    "    topic_embedding = np.mean(word_embeddings, axis=0).reshape(1, -1)\n",
    "    label_embeddings = model.encode(labels)\n",
    "    similarities = cosine_similarity(topic_embedding, label_embeddings)\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    return labels[most_similar_idx]\n",
    "\n",
    "\n",
    "def assign_labels_to_topics(topics, model, labels):\n",
    "    \"\"\"Assign labels to topics based on their top words using SentenceTransformer embeddings.\"\"\"\n",
    "    topic_labels = {}\n",
    "    for topic_name, top_words in topics.items():\n",
    "        label = get_topic_label(top_words, model, labels)\n",
    "        topic_labels[topic_name] = label\n",
    "    return topic_labels\n",
    "\n",
    "# Enter filename\n",
    "\n",
    "def visualize_topics_using_dictionary(topics, lda, vectorizer, topic_labels, filename=\"/Users/bzdrazil/Dropbox/EMBL_EBI/Papers/15year_ChEMBL/topic_word_heatmap_2010-19_regularised_2.png\"):\n",
    "    \"\"\"Visualize topics using the given dictionary of topics and save the heatmap as a file.\"\"\"\n",
    "    topic_word_distributions = lda.components_ / lda.components_.sum(axis=1)[:, None]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Extract the top words and their weights for each topic\n",
    "    data = []\n",
    "    for topic_idx, top_words in enumerate(topics.values()):\n",
    "        row = {word: topic_word_distributions[topic_idx][feature_names.tolist().index(word)]\n",
    "               for word in top_words if word in feature_names}\n",
    "        data.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    heatmap_data = pd.DataFrame(data).fillna(0)\n",
    "    # Use topic labels as y-axis labels\n",
    "    heatmap_data.index = [topic_labels.get(f\"Topic {i+1}\", f\"Topic {i+1}\") for i in range(len(topics))]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(heatmap_data, cmap=\"YlGnBu\", cbar_kws={'label': 'Word Weight'})\n",
    "    plt.title(\"Topic-Word Heatmap for papers 2010-2019\", fontsize = 16)\n",
    "    plt.xlabel(\"Words\", fontsize = 14)\n",
    "    plt.ylabel(\"Topics\", fontsize = 14)\n",
    "    plt.yticks(rotation=0)  # Rotate y-axis labels to horizontal\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the heatmap to a file\n",
    "    plt.savefig(filename, dpi = 600)\n",
    "    print(f\"Heatmap saved as {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    search_term = \"ChEMBL\"\n",
    "    max_results = 1000\n",
    "\n",
    "    # List of words to exclude\n",
    "    exclude_words = [\"exploring\", \"based\", \"strategy\",\"used\", \"using\", \"novel\", \"background\", \"chembl\", \"analysis\", \"study\", \"new\", \"research\", \"set\", \"known\", \"identification\"]\n",
    "\n",
    "    print(\"Searching PubMed...\")\n",
    "    pubmed_ids = search_pubmed(search_term, max_results)\n",
    "\n",
    "    print(f\"Found {len(pubmed_ids)} papers. Fetching details...\")\n",
    "    records = fetch_pubmed_details(pubmed_ids)\n",
    "\n",
    "    print(\"Extracting titles and abstracts...\")\n",
    "    papers = extract_titles_and_abstracts(records)\n",
    "\n",
    "    print(f\"Preprocessing {len(papers)} papers...\")\n",
    "    # Pass the list of exclude words to the preprocess_text function\n",
    "    preprocessed_papers = preprocess_text(papers, exclude_words=exclude_words)\n",
    "\n",
    "    print(\"Performing topic modeling...\")\n",
    "    lda, tfidf_matrix, vectorizer, topics = perform_topic_modeling(preprocessed_papers)\n",
    "\n",
    "    print(\"Identified Topics:\")\n",
    "    for topic, words in topics.items():\n",
    "        print(f\"{topic}: {', '.join(words)}\")\n",
    "\n",
    "    print(\"Generating single-word topic labels using SentenceTransformer...\")\n",
    "    topic_labels = assign_labels_to_topics(topics, model, labels)\n",
    "    for topic, label in topic_labels.items():\n",
    "        print(f\"{topic}: {label}\")\n",
    "\n",
    "    print(\"Visualizing topics...\")\n",
    "    # Pass topic_labels to the visualize function\n",
    "    visualize_topics_using_dictionary(topics, lda, vectorizer, topic_labels, filename=\"/Users/bzdrazil/Dropbox/EMBL_EBI/Papers/15year_ChEMBL/topic_word_heatmap_2010-19_regularised_2.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da48ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
